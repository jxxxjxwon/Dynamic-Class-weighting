# -*- coding: utf-8 -*-
"""Simple_CNN_parameter_0.3M(방법론 적용)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X8x5KL2cy8fkaxhyLTOOch1uE0F-WffM

https://github.com/sethuram-21/Mixed-Type-Wafer-Defect-Classification/blob/main/Attention-Augmented-CNN.ipynb
"""

import torch
!pip install split-folders

import splitfolders

input = "data"
output = "data split"

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
data = np.load('/content/drive/MyDrive/dataset/Wafer_Map_Datasets.npz')
print(data.files)  # 파일 내 저장된 모든 배열명 리스트 출력

import numpy as np
data = np.load('/content/drive/MyDrive/dataset/Wafer_Map_Datasets.npz')

# 내부 키 확인
print(data.files)  # ['arr_0', 'arr_1']

# 배열 불러오기
images = data['arr_0']
labels = data['arr_1']

import numpy as np
from PIL import Image
import os

data = np.load('/content/drive/MyDrive/dataset/Wafer_Map_Datasets.npz')
images = data['arr_0']   # (num_samples, H, W) or (num_samples, H, W, C)
labels = data['arr_1']   # (num_samples,)

for i, (img, label) in enumerate(zip(images, labels)):
    out_dir = f'data/class{label}'
    os.makedirs(out_dir, exist_ok=True)
    Image.fromarray(img).save(f'{out_dir}/{i}.png')

splitfolders.ratio(input, output=output, seed=1337, ratio=(0.72,0.08,0.2))

import os
import torch
import torchvision
import torchvision.transforms as transforms

os.listdir('./data split/train')

training_dataset_path="./data split/train"

training_transforms = transforms.Compose([transforms.Resize((52,52)), transforms.ToTensor()])

train_dataset = torchvision.datasets.ImageFolder( root = training_dataset_path, transform = training_transforms)

train_loader = torch.utils.data.DataLoader( dataset = train_dataset, batch_size = 32 , shuffle=False)

def get_mean_std(loader):
    # Compute the mean and standard deviation of all pixels in the dataset
    num_pixels = 0
    mean = 0.0
    std = 0.0
    for images, _ in loader:
        batch_size, num_channels, height, width = images.shape
        num_pixels += batch_size * height * width
        mean += images.mean(axis=(0, 2, 3)).sum()
        std += images.std(axis=(0, 2, 3)).sum()

    mean /= num_pixels
    std /= num_pixels

    return mean, std

mean,std = get_mean_std(train_loader)

import matplotlib.pyplot as plt
import numpy as np

train_dataset_path = './data split/train'
test_dataset_path = './data split/test'
val_dataset_path = './data split/val'

train_transforms = transforms.Compose([
    transforms.Resize((52,52)),
    transforms.Grayscale(num_output_channels=1),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(torch.Tensor(mean), torch.Tensor(std))
])

test_transforms = transforms.Compose([
    transforms.Resize((52,52)),
    transforms.Grayscale(num_output_channels=1),
    transforms.ToTensor(),
    transforms.Normalize(torch.Tensor(mean), torch.Tensor(std))
])

val_transforms = transforms.Compose([
    transforms.Resize((52,52)),
    transforms.Grayscale(num_output_channels=1),
    transforms.ToTensor(),
    transforms.Normalize(torch.Tensor(mean), torch.Tensor(std))
])

train_dataset = torchvision.datasets.ImageFolder(root = train_dataset_path, transform = train_transforms)
test_dataset = torchvision.datasets.ImageFolder(root = test_dataset_path, transform = test_transforms)
val_dataset = torchvision.datasets.ImageFolder(root = val_dataset_path, transform = val_transforms)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)

def set_device():
    if torch.cuda.is_available():
        dev = "cuda"
    else :
        dev = "cpu"
    return torch.device(dev)

"""#recall"""

# -*- coding: utf-8 -*-
"""
Simple CNN Baseline for MixedWM38 (Dynamic Weighted Version)
- ImprovedConfusionGuidedLoss (recall / accuracy / f1 기반)
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm
from sklearn.metrics import confusion_matrix, precision_recall_fscore_support

# ============================================
# 1. Simple CNN (~200K)
# ============================================
class SimpleCNN(nn.Module):
    """
    약 0.30M 파라미터 CNN (정확히 ~0.31M)
    """
    def __init__(self, num_classes=38):
        super().__init__()

        self.features = nn.Sequential(
            nn.Conv2d(1, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),  # 52 -> 26

            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),  # 26 -> 13

            nn.Conv2d(128, 160, 3, padding=1),  # 192 → 160으로 감소
            nn.BatchNorm2d(160),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),  # 13 -> 6
        )

        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(160, 256),  # 160 → 256
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes)
        )

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x


# ============================================
# 2. ImprovedConfusionGuidedLoss
# ============================================
class ImprovedConfusionGuidedLoss(nn.Module):
    def __init__(self, num_classes=38, metric='recall', gamma=2.0, epsilon=1e-8):
        super().__init__()
        self.num_classes = num_classes
        self.metric = metric
        self.gamma = gamma
        self.epsilon = epsilon
        self.register_buffer('class_weights', torch.ones(num_classes))

    def update_weights(self, y_true, y_pred):
        """선택된 metric(recall / accuracy / f1)에 따라 가중치 갱신"""
        cm = confusion_matrix(y_true, y_pred, labels=np.arange(self.num_classes))
        recall = np.diag(cm) / (cm.sum(axis=1) + self.epsilon)
        acc = np.diag(cm) / (cm.sum(axis=0) + self.epsilon)
        precision, _, f1, _ = precision_recall_fscore_support(
            y_true, y_pred, labels=np.arange(self.num_classes), zero_division=0
        )

        if self.metric == 'recall':
            metric_values = recall
        elif self.metric == 'accuracy':
            metric_values = acc
        elif self.metric == 'f1':
            metric_values = f1
        else:
            raise ValueError(f"Unknown metric type: {self.metric}")

        new_weights = 1.0 / (metric_values + self.epsilon)
        new_weights = np.clip(new_weights, 0.1, 10.0)
        new_weights /= np.mean(new_weights)
        self.class_weights = torch.tensor(new_weights, dtype=torch.float32, device=self.class_weights.device)
        print(f"[{self.metric}] 가중치 업데이트 완료 → min={self.class_weights.min():.3f}, max={self.class_weights.max():.3f}")

    def forward(self, inputs, targets):
        ce = nn.functional.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce)
        alpha = self.class_weights[targets]
        focal = alpha * (1 - pt) ** self.gamma * ce
        return focal.mean()


# ============================================
# 3. Early Stopping
# ============================================
class EarlyStopping:
    def __init__(self, patience=7, min_delta=0.001, verbose=True):
        self.patience = patience
        self.min_delta = min_delta
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.val_loss_min = float('inf')

    def __call__(self, val_loss, model, path):
        score = -val_loss
        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(val_loss, model, path)
        elif score < self.best_score + self.min_delta:
            self.counter += 1
            if self.verbose:
                print(f"EarlyStopping counter: {self.counter}/{self.patience}")
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(val_loss, model, path)
            self.counter = 0

    def save_checkpoint(self, val_loss, model, path):
        if self.verbose:
            print(f"Validation loss decreased ({self.val_loss_min:.6f} -> {val_loss:.6f}). Saving...")
        torch.save(model.state_dict(), path)
        self.val_loss_min = val_loss


# ============================================
# 4. Training & Validation
# ============================================
def evaluate_with_preds(model, loader, criterion, device):
    """손실, 정확도, 예측/정답 반환"""
    model.eval()
    running_loss, correct, total = 0.0, 0, 0
    all_preds, all_targets = [], []
    with torch.no_grad():
        for inputs, targets in loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
            all_preds.extend(predicted.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())
    acc = 100. * correct / total
    return running_loss / len(loader), acc, np.array(all_targets), np.array(all_preds)


# ============================================
# 5. Main
# ============================================
def main():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Device: {device}\n")

    train_path = './data split/train'
    val_path = './data split/val'
    test_path = './data split/test'

    # Transforms
    base_transforms = transforms.Compose([
        transforms.Resize((52, 52)),
        transforms.Grayscale(num_output_channels=1),
        transforms.ToTensor()
    ])
    train_transforms = transforms.Compose([
        transforms.Resize((52, 52)),
        transforms.Grayscale(num_output_channels=1),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor()
    ])

    # Datasets
    train_dataset = torchvision.datasets.ImageFolder(train_path, transform=train_transforms)
    val_dataset = torchvision.datasets.ImageFolder(val_path, transform=base_transforms)
    test_dataset = torchvision.datasets.ImageFolder(test_path, transform=base_transforms)
    num_classes = len(train_dataset.classes)

    # Loaders
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

    # Model
    model = SimpleCNN(num_classes=num_classes).to(device)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"Total parameters: {total_params:,} ({total_params/1e6:.2f}M)\n")

    # Loss / Optimizer
    criterion = ImprovedConfusionGuidedLoss(num_classes=num_classes, metric='recall').to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)
    early_stopping = EarlyStopping(patience=7, verbose=True)

    best_val_acc = 0.0
    num_epochs = 50

    for epoch in range(num_epochs):
        print(f"\nEpoch {epoch+1}/{num_epochs}\n" + "-"*60)
        model.train()
        running_loss, correct, total = 0.0, 0, 0
        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

        train_acc = 100. * correct / total
        train_loss = running_loss / len(train_loader)

        # Validation + metric update
        val_loss, val_acc, y_true, y_pred = evaluate_with_preds(model, val_loader, criterion, device)
        criterion.update_weights(y_true, y_pred)
        scheduler.step()

        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), "best_dynamic_cnn.pth")

        print(f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%")
        print(f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | Best: {best_val_acc:.2f}%")

        early_stopping(val_loss, model, "early_stop_checkpoint.pth")
        if early_stopping.early_stop:
            print(f"\nEarly stopping at epoch {epoch+1}")
            break

    # Test
    print("\n" + "="*60)
    print("최종 테스트")
    print("="*60)
    model.load_state_dict(torch.load("best_dynamic_cnn.pth"))
    test_loss, test_acc, _, _ = evaluate_with_preds(model, test_loader, criterion, device)
    print(f"Test Accuracy: {test_acc:.2f}% | Best Val: {best_val_acc:.2f}%")
    print("훈련 완료 ✅")


if __name__ == '__main__':
    main()

"""f1"""

# -*- coding: utf-8 -*-
"""
Simple CNN Baseline for MixedWM38 (Dynamic Weighted Version)
- ImprovedConfusionGuidedLoss (recall / accuracy / f1 기반)
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm
from sklearn.metrics import confusion_matrix, precision_recall_fscore_support

# ============================================
# 1. Simple CNN (~200K)
# ============================================
class SimpleCNN(nn.Module):
    """
    약 0.30M 파라미터 CNN (정확히 ~0.31M)
    """
    def __init__(self, num_classes=38):
        super().__init__()

        self.features = nn.Sequential(
            nn.Conv2d(1, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),  # 52 -> 26

            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),  # 26 -> 13

            nn.Conv2d(128, 160, 3, padding=1),  # 192 → 160으로 감소
            nn.BatchNorm2d(160),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),  # 13 -> 6
        )

        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(160, 256),  # 160 → 256
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes)
        )

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x


# ============================================
# 2. ImprovedConfusionGuidedLoss
# ============================================
class ImprovedConfusionGuidedLoss(nn.Module):
    def __init__(self, num_classes=38, metric='f1', gamma=2.0, epsilon=1e-8):
        super().__init__()
        self.num_classes = num_classes
        self.metric = metric
        self.gamma = gamma
        self.epsilon = epsilon
        self.register_buffer('class_weights', torch.ones(num_classes))

    def update_weights(self, y_true, y_pred):
        """선택된 metric(recall / accuracy / f1)에 따라 가중치 갱신"""
        cm = confusion_matrix(y_true, y_pred, labels=np.arange(self.num_classes))
        recall = np.diag(cm) / (cm.sum(axis=1) + self.epsilon)
        acc = np.diag(cm) / (cm.sum(axis=0) + self.epsilon)
        precision, _, f1, _ = precision_recall_fscore_support(
            y_true, y_pred, labels=np.arange(self.num_classes), zero_division=0
        )

        if self.metric == 'recall':
            metric_values = recall
        elif self.metric == 'accuracy':
            metric_values = acc
        elif self.metric == 'f1':
            metric_values = f1
        else:
            raise ValueError(f"Unknown metric type: {self.metric}")

        new_weights = 1.0 / (metric_values + self.epsilon)
        new_weights = np.clip(new_weights, 0.1, 10.0)
        new_weights /= np.mean(new_weights)
        self.class_weights = torch.tensor(new_weights, dtype=torch.float32, device=self.class_weights.device)
        print(f"[{self.metric}] 가중치 업데이트 완료 → min={self.class_weights.min():.3f}, max={self.class_weights.max():.3f}")

    def forward(self, inputs, targets):
        ce = nn.functional.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce)
        alpha = self.class_weights[targets]
        focal = alpha * (1 - pt) ** self.gamma * ce
        return focal.mean()


# ============================================
# 3. Early Stopping
# ============================================
class EarlyStopping:
    def __init__(self, patience=7, min_delta=0.001, verbose=True):
        self.patience = patience
        self.min_delta = min_delta
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.val_loss_min = float('inf')

    def __call__(self, val_loss, model, path):
        score = -val_loss
        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(val_loss, model, path)
        elif score < self.best_score + self.min_delta:
            self.counter += 1
            if self.verbose:
                print(f"EarlyStopping counter: {self.counter}/{self.patience}")
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(val_loss, model, path)
            self.counter = 0

    def save_checkpoint(self, val_loss, model, path):
        if self.verbose:
            print(f"Validation loss decreased ({self.val_loss_min:.6f} -> {val_loss:.6f}). Saving...")
        torch.save(model.state_dict(), path)
        self.val_loss_min = val_loss


# ============================================
# 4. Training & Validation
# ============================================
def evaluate_with_preds(model, loader, criterion, device):
    """손실, 정확도, 예측/정답 반환"""
    model.eval()
    running_loss, correct, total = 0.0, 0, 0
    all_preds, all_targets = [], []
    with torch.no_grad():
        for inputs, targets in loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
            all_preds.extend(predicted.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())
    acc = 100. * correct / total
    return running_loss / len(loader), acc, np.array(all_targets), np.array(all_preds)


# ============================================
# 5. Main
# ============================================
def main():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Device: {device}\n")

    train_path = './data split/train'
    val_path = './data split/val'
    test_path = './data split/test'

    # Transforms
    base_transforms = transforms.Compose([
        transforms.Resize((52, 52)),
        transforms.Grayscale(num_output_channels=1),
        transforms.ToTensor()
    ])
    train_transforms = transforms.Compose([
        transforms.Resize((52, 52)),
        transforms.Grayscale(num_output_channels=1),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor()
    ])

    # Datasets
    train_dataset = torchvision.datasets.ImageFolder(train_path, transform=train_transforms)
    val_dataset = torchvision.datasets.ImageFolder(val_path, transform=base_transforms)
    test_dataset = torchvision.datasets.ImageFolder(test_path, transform=base_transforms)
    num_classes = len(train_dataset.classes)

    # Loaders
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

    # Model
    model = SimpleCNN(num_classes=num_classes).to(device)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"Total parameters: {total_params:,} ({total_params/1e6:.2f}M)\n")

    # Loss / Optimizer
    criterion = ImprovedConfusionGuidedLoss(num_classes=num_classes, metric='f1').to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)
    early_stopping = EarlyStopping(patience=7, verbose=True)

    best_val_acc = 0.0
    num_epochs = 50

    for epoch in range(num_epochs):
        print(f"\nEpoch {epoch+1}/{num_epochs}\n" + "-"*60)
        model.train()
        running_loss, correct, total = 0.0, 0, 0
        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

        train_acc = 100. * correct / total
        train_loss = running_loss / len(train_loader)

        # Validation + metric update
        val_loss, val_acc, y_true, y_pred = evaluate_with_preds(model, val_loader, criterion, device)
        criterion.update_weights(y_true, y_pred)
        scheduler.step()

        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), "best_dynamic_cnn.pth")

        print(f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%")
        print(f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | Best: {best_val_acc:.2f}%")

        early_stopping(val_loss, model, "early_stop_checkpoint.pth")
        if early_stopping.early_stop:
            print(f"\nEarly stopping at epoch {epoch+1}")
            break

    # Test
    print("\n" + "="*60)
    print("최종 테스트")
    print("="*60)
    model.load_state_dict(torch.load("best_dynamic_cnn.pth"))
    test_loss, test_acc, _, _ = evaluate_with_preds(model, test_loader, criterion, device)
    print(f"Test Accuracy: {test_acc:.2f}% | Best Val: {best_val_acc:.2f}%")
    print("훈련 완료 ✅")


if __name__ == '__main__':
    main()

"""#accuracy

"""

# -*- coding: utf-8 -*-
"""
Simple CNN Baseline for MixedWM38 (Dynamic Weighted Version)
- ImprovedConfusionGuidedLoss (recall / accuracy / f1 기반)
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm
from sklearn.metrics import confusion_matrix, precision_recall_fscore_support

# ============================================
# 1. Simple CNN (~200K)
# ============================================
class SimpleCNN(nn.Module):
    """
    약 0.30M 파라미터 CNN (정확히 ~0.31M)
    """
    def __init__(self, num_classes=38):
        super().__init__()

        self.features = nn.Sequential(
            nn.Conv2d(1, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),  # 52 -> 26

            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),  # 26 -> 13

            nn.Conv2d(128, 160, 3, padding=1),  # 192 → 160으로 감소
            nn.BatchNorm2d(160),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),  # 13 -> 6
        )

        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(160, 256),  # 160 → 256
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes)
        )

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x


# ============================================
# 2. ImprovedConfusionGuidedLoss
# ============================================
class ImprovedConfusionGuidedLoss(nn.Module):
    def __init__(self, num_classes=38, metric='accuracy', gamma=2.0, epsilon=1e-8):
        super().__init__()
        self.num_classes = num_classes
        self.metric = metric
        self.gamma = gamma
        self.epsilon = epsilon
        self.register_buffer('class_weights', torch.ones(num_classes))

    def update_weights(self, y_true, y_pred):
        """선택된 metric(recall / accuracy / f1)에 따라 가중치 갱신"""
        cm = confusion_matrix(y_true, y_pred, labels=np.arange(self.num_classes))
        recall = np.diag(cm) / (cm.sum(axis=1) + self.epsilon)
        acc = np.diag(cm) / (cm.sum(axis=0) + self.epsilon)
        precision, _, f1, _ = precision_recall_fscore_support(
            y_true, y_pred, labels=np.arange(self.num_classes), zero_division=0
        )

        if self.metric == 'recall':
            metric_values = recall
        elif self.metric == 'accuracy':
            metric_values = acc
        elif self.metric == 'f1':
            metric_values = f1
        else:
            raise ValueError(f"Unknown metric type: {self.metric}")

        new_weights = 1.0 / (metric_values + self.epsilon)
        new_weights = np.clip(new_weights, 0.1, 10.0)
        new_weights /= np.mean(new_weights)
        self.class_weights = torch.tensor(new_weights, dtype=torch.float32, device=self.class_weights.device)
        print(f"[{self.metric}] 가중치 업데이트 완료 → min={self.class_weights.min():.3f}, max={self.class_weights.max():.3f}")

    def forward(self, inputs, targets):
        ce = nn.functional.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce)
        alpha = self.class_weights[targets]
        focal = alpha * (1 - pt) ** self.gamma * ce
        return focal.mean()


# ============================================
# 3. Early Stopping
# ============================================
class EarlyStopping:
    def __init__(self, patience=7, min_delta=0.001, verbose=True):
        self.patience = patience
        self.min_delta = min_delta
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.val_loss_min = float('inf')

    def __call__(self, val_loss, model, path):
        score = -val_loss
        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(val_loss, model, path)
        elif score < self.best_score + self.min_delta:
            self.counter += 1
            if self.verbose:
                print(f"EarlyStopping counter: {self.counter}/{self.patience}")
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(val_loss, model, path)
            self.counter = 0

    def save_checkpoint(self, val_loss, model, path):
        if self.verbose:
            print(f"Validation loss decreased ({self.val_loss_min:.6f} -> {val_loss:.6f}). Saving...")
        torch.save(model.state_dict(), path)
        self.val_loss_min = val_loss


# ============================================
# 4. Training & Validation
# ============================================
def evaluate_with_preds(model, loader, criterion, device):
    """손실, 정확도, 예측/정답 반환"""
    model.eval()
    running_loss, correct, total = 0.0, 0, 0
    all_preds, all_targets = [], []
    with torch.no_grad():
        for inputs, targets in loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
            all_preds.extend(predicted.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())
    acc = 100. * correct / total
    return running_loss / len(loader), acc, np.array(all_targets), np.array(all_preds)


# ============================================
# 5. Main
# ============================================
def main():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Device: {device}\n")

    train_path = './data split/train'
    val_path = './data split/val'
    test_path = './data split/test'

    # Transforms
    base_transforms = transforms.Compose([
        transforms.Resize((52, 52)),
        transforms.Grayscale(num_output_channels=1),
        transforms.ToTensor()
    ])
    train_transforms = transforms.Compose([
        transforms.Resize((52, 52)),
        transforms.Grayscale(num_output_channels=1),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor()
    ])

    # Datasets
    train_dataset = torchvision.datasets.ImageFolder(train_path, transform=train_transforms)
    val_dataset = torchvision.datasets.ImageFolder(val_path, transform=base_transforms)
    test_dataset = torchvision.datasets.ImageFolder(test_path, transform=base_transforms)
    num_classes = len(train_dataset.classes)

    # Loaders
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

    # Model
    model = SimpleCNN(num_classes=num_classes).to(device)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"Total parameters: {total_params:,} ({total_params/1e6:.2f}M)\n")

    # Loss / Optimizer
    criterion = ImprovedConfusionGuidedLoss(num_classes=num_classes, metric='accuracy').to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)
    early_stopping = EarlyStopping(patience=7, verbose=True)

    best_val_acc = 0.0
    num_epochs = 50

    for epoch in range(num_epochs):
        print(f"\nEpoch {epoch+1}/{num_epochs}\n" + "-"*60)
        model.train()
        running_loss, correct, total = 0.0, 0, 0
        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

        train_acc = 100. * correct / total
        train_loss = running_loss / len(train_loader)

        # Validation + metric update
        val_loss, val_acc, y_true, y_pred = evaluate_with_preds(model, val_loader, criterion, device)
        criterion.update_weights(y_true, y_pred)
        scheduler.step()

        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), "best_dynamic_cnn.pth")

        print(f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%")
        print(f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | Best: {best_val_acc:.2f}%")

        early_stopping(val_loss, model, "early_stop_checkpoint.pth")
        if early_stopping.early_stop:
            print(f"\nEarly stopping at epoch {epoch+1}")
            break

    # Test
    print("\n" + "="*60)
    print("최종 테스트")
    print("="*60)
    model.load_state_dict(torch.load("best_dynamic_cnn.pth"))
    test_loss, test_acc, _, _ = evaluate_with_preds(model, test_loader, criterion, device)
    print(f"Test Accuracy: {test_acc:.2f}% | Best Val: {best_val_acc:.2f}%")
    print("훈련 완료 ✅")


if __name__ == '__main__':
    main()

"""수정 버전...."""

# -*- coding: utf-8 -*-
"""
Simple CNN Baseline for MixedWM38 (Dynamic Weighted + Normalized)
- ImprovedConfusionGuidedLoss (recall / accuracy / f1 기반)
- Grayscale 이미지 정규화 적용
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm
from sklearn.metrics import confusion_matrix, precision_recall_fscore_support

# ============================================
# 1. Simple CNN (~0.31M)
# ============================================
class SimpleCNN(nn.Module):
    def __init__(self, num_classes=38):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(1, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),

            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),

            nn.Conv2d(128, 160, 3, padding=1),
            nn.BatchNorm2d(160),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
        )
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(160, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes)
        )

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x

# ============================================
# 2. ImprovedConfusionGuidedLoss
# ============================================
class ImprovedConfusionGuidedLoss(nn.Module):
    def __init__(self, num_classes=38, metric='accuracy', gamma=2.0, epsilon=1e-8):
        super().__init__()
        self.num_classes = num_classes
        self.metric = metric
        self.gamma = gamma
        self.epsilon = epsilon
        self.register_buffer('class_weights', torch.ones(num_classes))

    def update_weights(self, y_true, y_pred):
        cm = confusion_matrix(y_true, y_pred, labels=np.arange(self.num_classes))
        recall = np.diag(cm) / (cm.sum(axis=1) + self.epsilon)
        acc = np.diag(cm) / (cm.sum(axis=0) + self.epsilon)
        precision, _, f1, _ = precision_recall_fscore_support(
            y_true, y_pred, labels=np.arange(self.num_classes), zero_division=0
        )

        if self.metric == 'recall':
            metric_values = recall
        elif self.metric == 'accuracy':
            metric_values = acc
        elif self.metric == 'f1':
            metric_values = f1
        else:
            raise ValueError(f"Unknown metric type: {self.metric}")

        new_weights = 1.0 / (metric_values + self.epsilon)
        new_weights = np.clip(new_weights, 0.1, 10.0)
        new_weights /= np.mean(new_weights)
        self.class_weights = torch.tensor(new_weights, dtype=torch.float32, device=self.class_weights.device)
        print(f"[{self.metric}] 가중치 업데이트 완료 → min={self.class_weights.min():.3f}, max={self.class_weights.max():.3f}")

    def forward(self, inputs, targets):
        ce = nn.functional.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce)
        alpha = self.class_weights[targets]
        focal = alpha * (1 - pt) ** self.gamma * ce
        return focal.mean()

# ============================================
# 3. EarlyStopping
# ============================================
class EarlyStopping:
    def __init__(self, patience=7, min_delta=0.001, verbose=True):
        self.patience = patience
        self.min_delta = min_delta
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.val_loss_min = float('inf')

    def __call__(self, val_loss, model, path):
        score = -val_loss
        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(val_loss, model, path)
        elif score < self.best_score + self.min_delta:
            self.counter += 1
            if self.verbose:
                print(f"EarlyStopping counter: {self.counter}/{self.patience}")
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(val_loss, model, path)
            self.counter = 0

    def save_checkpoint(self, val_loss, model, path):
        if self.verbose:
            print(f"Validation loss decreased ({self.val_loss_min:.6f} -> {val_loss:.6f}). Saving...")
        torch.save(model.state_dict(), path)
        self.val_loss_min = val_loss

# ============================================
# 4. Mean/Std 계산 함수
# ============================================
def get_mean_std(loader):
    mean = 0.0
    std = 0.0
    total_images = 0
    for images, _ in loader:
        images = images.view(images.size(0), -1)
        mean += images.mean(1).sum()
        std += images.std(1).sum()
        total_images += images.size(0)
    mean /= total_images
    std /= total_images
    return mean.item(), std.item()

# ============================================
# 5. Evaluate 함수
# ============================================
def evaluate_with_preds(model, loader, criterion, device):
    model.eval()
    running_loss, correct, total = 0.0, 0, 0
    all_preds, all_targets = [], []
    with torch.no_grad():
        for inputs, targets in loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
            all_preds.extend(predicted.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())
    acc = 100. * correct / total
    return running_loss / len(loader), acc, np.array(all_targets), np.array(all_preds)

# ============================================
# 6. Main
# ============================================
def main():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Device: {device}\n")

    train_path = './data split/train'
    val_path = './data split/val'
    test_path = './data split/test'

    # Base transform (정규화는 추후 계산)
    base_transforms = transforms.Compose([
        transforms.Resize((52, 52)),
        transforms.Grayscale(num_output_channels=1),
        transforms.ToTensor()
    ])
    train_transforms = transforms.Compose([
        transforms.Resize((52, 52)),
        transforms.Grayscale(num_output_channels=1),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor()
    ])

    # Datasets & Loaders
    train_dataset = torchvision.datasets.ImageFolder(train_path, transform=train_transforms)
    val_dataset = torchvision.datasets.ImageFolder(val_path, transform=base_transforms)
    test_dataset = torchvision.datasets.ImageFolder(test_path, transform=base_transforms)
    num_classes = len(train_dataset.classes)

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

    # Mean/Std 계산 후 정규화 적용
    train_mean, train_std = get_mean_std(train_loader)
    print(f"Train Mean: {train_mean:.4f}, Std: {train_std:.4f}")

    # 정규화 반영
    train_dataset.transform = transforms.Compose([
        transforms.Resize((52, 52)),
        transforms.Grayscale(num_output_channels=1),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(mean=[train_mean], std=[train_std])
    ])
    val_dataset.transform = transforms.Compose([
        transforms.Resize((52, 52)),
        transforms.Grayscale(num_output_channels=1),
        transforms.ToTensor(),
        transforms.Normalize(mean=[train_mean], std=[train_std])
    ])
    test_dataset.transform = val_dataset.transform

    # Model, Loss, Optimizer
    model = SimpleCNN(num_classes=num_classes).to(device)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"Total parameters: {total_params:,} ({total_params/1e6:.2f}M)\n")

    criterion = ImprovedConfusionGuidedLoss(num_classes=num_classes, metric='accuracy').to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)
    early_stopping = EarlyStopping(patience=7, verbose=True)

    best_val_acc = 0.0
    num_epochs = 50

    for epoch in range(num_epochs):
        print(f"\nEpoch {epoch+1}/{num_epochs}\n" + "-"*60)
        model.train()
        running_loss, correct, total = 0.0, 0, 0
        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

        train_acc = 100. * correct / total
        train_loss = running_loss / len(train_loader)

        # Validation + metric update
        val_loss, val_acc, y_true, y_pred = evaluate_with_preds(model, val_loader, criterion, device)
        criterion.update_weights(y_true, y_pred)
        scheduler.step()

        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), "best_dynamic_cnn.pth")

        print(f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%")
        print(f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | Best: {best_val_acc:.2f}%")

        early_stopping(val_loss, model, "early_stop_checkpoint.pth")
        if early_stopping.early_stop:
            print(f"\nEarly stopping at epoch {epoch+1}")
            break

    # Test
    print("\n" + "="*60)
    print("최종 테스트")
    print("="*60)
    model.load_state_dict(torch.load("best_dynamic_cnn.pth"))
    test_loss, test_acc, _, _ = evaluate_with_preds(model, test_loader, criterion, device)
    print(f"Test Accuracy: {test_acc:.2f}% | Best Val: {best_val_acc:.2f}%")
    print("훈련 완료 ✅")

if __name__ == '__main__':
    main()

# -*- coding: utf-8 -*-
"""
Simple CNN Baseline for MixedWM38 (Dynamic Weighted + Normalized)
- ImprovedConfusionGuidedLoss (recall / accuracy / f1 기반)
- Grayscale 이미지 정규화 적용
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm
from sklearn.metrics import confusion_matrix, precision_recall_fscore_support

# ============================================
# 1. Simple CNN (~0.31M)
# ============================================
class SimpleCNN(nn.Module):
    def __init__(self, num_classes=38):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(1, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),

            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),

            nn.Conv2d(128, 160, 3, padding=1),
            nn.BatchNorm2d(160),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),
        )
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(160, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.5),
            nn.Linear(256, num_classes)
        )

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x

# ============================================
# 2. ImprovedConfusionGuidedLoss
# ============================================
class ImprovedConfusionGuidedLoss(nn.Module):
    def __init__(self, num_classes=38, metric='accuracy', gamma=2.0, epsilon=1e-8):
        super().__init__()
        self.num_classes = num_classes
        self.metric = metric
        self.gamma = gamma
        self.epsilon = epsilon
        self.register_buffer('class_weights', torch.ones(num_classes))

    def update_weights(self, y_true, y_pred):
        cm = confusion_matrix(y_true, y_pred, labels=np.arange(self.num_classes))
        recall = np.diag(cm) / (cm.sum(axis=1) + self.epsilon)
        acc = np.diag(cm) / (cm.sum(axis=0) + self.epsilon)
        precision, _, f1, _ = precision_recall_fscore_support(
            y_true, y_pred, labels=np.arange(self.num_classes), zero_division=0
        )

        if self.metric == 'recall':
            metric_values = recall
        elif self.metric == 'accuracy':
            metric_values = acc
        elif self.metric == 'f1':
            metric_values = f1
        else:
            raise ValueError(f"Unknown metric type: {self.metric}")

        new_weights = 1.0 / (metric_values + self.epsilon)
        new_weights = np.clip(new_weights, 0.1, 10.0)
        new_weights /= np.mean(new_weights)
        self.class_weights = torch.tensor(new_weights, dtype=torch.float32, device=self.class_weights.device)
        print(f"[{self.metric}] 가중치 업데이트 완료 → min={self.class_weights.min():.3f}, max={self.class_weights.max():.3f}")

    def forward(self, inputs, targets):
        ce = nn.functional.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce)
        alpha = self.class_weights[targets]
        focal = alpha * (1 - pt) ** self.gamma * ce
        return focal.mean()

# ============================================
# 3. EarlyStopping
# ============================================
class EarlyStopping:
    def __init__(self, patience=7, min_delta=0.001, verbose=True):
        self.patience = patience
        self.min_delta = min_delta
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.val_loss_min = float('inf')

    def __call__(self, val_loss, model, path):
        score = -val_loss
        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(val_loss, model, path)
        elif score < self.best_score + self.min_delta:
            self.counter += 1
            if self.verbose:
                print(f"EarlyStopping counter: {self.counter}/{self.patience}")
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(val_loss, model, path)
            self.counter = 0

    def save_checkpoint(self, val_loss, model, path):
        if self.verbose:
            print(f"Validation loss decreased ({self.val_loss_min:.6f} -> {val_loss:.6f}). Saving...")
        torch.save(model.state_dict(), path)
        self.val_loss_min = val_loss

# ============================================
# 4. Mean/Std 계산 함수
# ============================================
def get_mean_std(loader):
    mean = 0.0
    std = 0.0
    total_images = 0
    for images, _ in loader:
        images = images.view(images.size(0), -1)
        mean += images.mean(1).sum()
        std += images.std(1).sum()
        total_images += images.size(0)
    mean /= total_images
    std /= total_images
    return mean.item(), std.item()

# ============================================
# 5. Evaluate 함수
# ============================================
def evaluate_with_preds(model, loader, criterion, device):
    model.eval()
    running_loss, correct, total = 0.0, 0, 0
    all_preds, all_targets = [], []
    with torch.no_grad():
        for inputs, targets in loader:
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
            all_preds.extend(predicted.cpu().numpy())
            all_targets.extend(targets.cpu().numpy())
    acc = 100. * correct / total
    return running_loss / len(loader), acc, np.array(all_targets), np.array(all_preds)

# ============================================
# 6. Main
# ============================================
def main():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Device: {device}\n")

    train_path = './data split/train'
    val_path = './data split/val'
    test_path = './data split/test'

    # Base transform (정규화는 추후 계산)
    base_transforms = transforms.Compose([
        transforms.Resize((52, 52)),
        transforms.Grayscale(num_output_channels=1),
        transforms.ToTensor()
    ])
    train_transforms = transforms.Compose([
        transforms.Resize((52, 52)),
        transforms.Grayscale(num_output_channels=1),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor()
    ])

    # Datasets & Loaders
    train_dataset = torchvision.datasets.ImageFolder(train_path, transform=train_transforms)
    val_dataset = torchvision.datasets.ImageFolder(val_path, transform=base_transforms)
    test_dataset = torchvision.datasets.ImageFolder(test_path, transform=base_transforms)
    num_classes = len(train_dataset.classes)

    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

    # Mean/Std 계산 후 정규화 적용
    train_mean, train_std = get_mean_std(train_loader)
    print(f"Train Mean: {train_mean:.4f}, Std: {train_std:.4f}")

    # 정규화 반영
    train_dataset.transform = transforms.Compose([
        transforms.Resize((52, 52)),
        transforms.Grayscale(num_output_channels=1),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(mean=[train_mean], std=[train_std])
    ])
    val_dataset.transform = transforms.Compose([
        transforms.Resize((52, 52)),
        transforms.Grayscale(num_output_channels=1),
        transforms.ToTensor(),
        transforms.Normalize(mean=[train_mean], std=[train_std])
    ])
    test_dataset.transform = val_dataset.transform

    # Model, Loss, Optimizer
    model = SimpleCNN(num_classes=num_classes).to(device)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"Total parameters: {total_params:,} ({total_params/1e6:.2f}M)\n")

    criterion = ImprovedConfusionGuidedLoss(num_classes=num_classes, metric='f1').to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)
    early_stopping = EarlyStopping(patience=7, verbose=True)

    best_val_acc = 0.0
    num_epochs = 50

    for epoch in range(num_epochs):
        print(f"\nEpoch {epoch+1}/{num_epochs}\n" + "-"*60)
        model.train()
        running_loss, correct, total = 0.0, 0, 0
        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

        train_acc = 100. * correct / total
        train_loss = running_loss / len(train_loader)

        # Validation + metric update
        val_loss, val_acc, y_true, y_pred = evaluate_with_preds(model, val_loader, criterion, device)
        criterion.update_weights(y_true, y_pred)
        scheduler.step()

        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), "best_dynamic_cnn.pth")

        print(f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%")
        print(f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | Best: {best_val_acc:.2f}%")

        early_stopping(val_loss, model, "early_stop_checkpoint.pth")
        if early_stopping.early_stop:
            print(f"\nEarly stopping at epoch {epoch+1}")
            break

    # Test
    print("\n" + "="*60)
    print("최종 테스트")
    print("="*60)
    model.load_state_dict(torch.load("best_dynamic_cnn.pth"))
    test_loss, test_acc, _, _ = evaluate_with_preds(model, test_loader, criterion, device)
    print(f"Test Accuracy: {test_acc:.2f}% | Best Val: {best_val_acc:.2f}%")
    print("훈련 완료 ✅")

if __name__ == '__main__':
    main()